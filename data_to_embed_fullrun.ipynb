{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for training\n",
    "\n",
    "- Create a dataset from raw data:\n",
    "  - (doc, query, relevance)\n",
    "  - relevance is 0 (not relevant), 1 (relevant), 2 (user-selected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting polars\n",
      "  Using cached polars-1.10.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Using cached polars-1.10.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.2 MB)\n",
      "Installing collected packages: polars\n",
      "Successfully installed polars-1.10.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# from datasets import load_dataset\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import collections\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd7165fc17624083809f307ac524a2c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/9.48k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kaleb\\Code\\TwoTowerSearch\\env\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\kaleb\\.cache\\huggingface\\hub\\datasets--ms_marco. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d1c6d51a77840249eafae5e2fd82fcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/21.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fbef68a22fb420199e72b468da59e18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/175M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ced538d1234bdbba25b61affb8b9c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6be807630a9e4ddeb82e4b7518dcb5fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10047 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "719eea254a114d2fbc8da744701f3967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/82326 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d8cdf7432254bc5baf153ec079fc80d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/9650 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data = load_dataset(\"ms_marco\", \"v1.1\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess(text: str) -> list[str]:\n",
    "#     text = text.lower()\n",
    "#     text = text.replace(\".\", \" <PERIOD> \")\n",
    "#     text = text.replace(\",\", \" <COMMA> \")\n",
    "#     text = text.replace('\"', \" <QUOTATION_MARK> \")\n",
    "#     text = text.replace(\";\", \" <SEMICOLON> \")\n",
    "#     text = text.replace(\"!\", \" <EXCLAMATION_MARK> \")\n",
    "#     text = text.replace(\"?\", \" <QUESTION_MARK> \")\n",
    "#     text = text.replace(\"(\", \" <LEFT_PAREN> \")\n",
    "#     text = text.replace(\")\", \" <RIGHT_PAREN> \")\n",
    "#     text = text.replace(\"--\", \" <HYPHENS> \")\n",
    "#     text = text.replace(\"?\", \" <QUESTION_MARK> \")\n",
    "#     text = text.replace(\":\", \" <COLON> \")\n",
    "#     words = text.split()\n",
    "#     stats = collections.Counter(words)\n",
    "#     words = [word for word in words if stats[word] > 5]\n",
    "#     return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_raw = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answers</th>\n",
       "      <th>passages</th>\n",
       "      <th>query</th>\n",
       "      <th>query_id</th>\n",
       "      <th>query_type</th>\n",
       "      <th>wellFormedAnswers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Results-Based Accountability is a disciplined...</td>\n",
       "      <td>{'is_selected': [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]...</td>\n",
       "      <td>what is rba</td>\n",
       "      <td>19699</td>\n",
       "      <td>description</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Yes]</td>\n",
       "      <td>{'is_selected': [0, 1, 0, 0, 0, 0, 0], 'passag...</td>\n",
       "      <td>was ronald reagan a democrat</td>\n",
       "      <td>19700</td>\n",
       "      <td>description</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[20-25 minutes]</td>\n",
       "      <td>{'is_selected': [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]...</td>\n",
       "      <td>how long do you need for sydney and surroundin...</td>\n",
       "      <td>19701</td>\n",
       "      <td>numeric</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[$11 to $22 per square foot]</td>\n",
       "      <td>{'is_selected': [0, 0, 0, 0, 0, 0, 0, 0, 1], '...</td>\n",
       "      <td>price to install tile in shower</td>\n",
       "      <td>19702</td>\n",
       "      <td>numeric</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Due to symptoms in the body]</td>\n",
       "      <td>{'is_selected': [0, 0, 1, 0, 0, 0, 0, 0], 'pas...</td>\n",
       "      <td>why conversion observed in body</td>\n",
       "      <td>19703</td>\n",
       "      <td>description</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             answers  \\\n",
       "0  [Results-Based Accountability is a disciplined...   \n",
       "1                                              [Yes]   \n",
       "2                                    [20-25 minutes]   \n",
       "3                       [$11 to $22 per square foot]   \n",
       "4                      [Due to symptoms in the body]   \n",
       "\n",
       "                                            passages  \\\n",
       "0  {'is_selected': [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]...   \n",
       "1  {'is_selected': [0, 1, 0, 0, 0, 0, 0], 'passag...   \n",
       "2  {'is_selected': [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]...   \n",
       "3  {'is_selected': [0, 0, 0, 0, 0, 0, 0, 0, 1], '...   \n",
       "4  {'is_selected': [0, 0, 1, 0, 0, 0, 0, 0], 'pas...   \n",
       "\n",
       "                                               query  query_id   query_type  \\\n",
       "0                                        what is rba     19699  description   \n",
       "1                       was ronald reagan a democrat     19700  description   \n",
       "2  how long do you need for sydney and surroundin...     19701      numeric   \n",
       "3                    price to install tile in shower     19702      numeric   \n",
       "4                    why conversion observed in body     19703  description   \n",
       "\n",
       "  wellFormedAnswers  \n",
       "0                []  \n",
       "1                []  \n",
       "2                []  \n",
       "3                []  \n",
       "4                []  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def unwrap_passages(row: pd.Series) -> pd.DataFrame:\n",
    "#     df_psg = pd.DataFrame(row[\"passages\"])\n",
    "#     df_psg[\"query\"] = row[\"query\"]\n",
    "#     df_psg[\"query_id\"] = row[\"query_id\"]\n",
    "#     df_psg[\"query_type\"] = row[\"query_type\"]\n",
    "#     # answers = row['answers']\n",
    "#     # df_psg['answers'] = [answers] * len(df_psg) if answers and any(answers) else [[]] * len(df_psg)\n",
    "#     return df_psg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunks = []\n",
    "# for i, row in df_raw.iterrows():\n",
    "#     chunks.append(unwrap_passages(row))\n",
    "# df = pd.concat(chunks).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_selected</th>\n",
       "      <th>passage_text</th>\n",
       "      <th>url</th>\n",
       "      <th>query</th>\n",
       "      <th>query_id</th>\n",
       "      <th>query_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Since 2007, the RBA's outstanding reputation h...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Reserve_Bank_of_...</td>\n",
       "      <td>what is rba</td>\n",
       "      <td>19699</td>\n",
       "      <td>description</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_selected                                       passage_text  \\\n",
       "0            0  Since 2007, the RBA's outstanding reputation h...   \n",
       "\n",
       "                                                 url        query  query_id  \\\n",
       "0  https://en.wikipedia.org/wiki/Reserve_Bank_of_...  what is rba     19699   \n",
       "\n",
       "    query_type  \n",
       "0  description  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # save df to data/processed/flattened_train.parquet\n",
    "# file_path = os.path.join(\"data\", \"processed\", \"flattened_train.parquet\")\n",
    "# df.to_parquet(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import df from data/processed/flattened_train.parquet\n",
    "file_path = os.path.join(\"flattened_train.parquet\")\n",
    "df = pd.read_parquet(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "676193"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0.0% completed\n",
      "Progress: 10.0% completed\n",
      "Progress: 20.0% completed\n",
      "Progress: 30.0% completed\n",
      "Progress: 40.0% completed\n",
      "Progress: 50.0% completed\n",
      "Progress: 60.0% completed\n",
      "Progress: 70.0% completed\n",
      "Progress: 80.0% completed\n",
      "Progress: 90.0% completed\n",
      "Progress: 100.0% completed\n"
     ]
    }
   ],
   "source": [
    "# Rename columns for clarity\n",
    "dfc = dfc.rename({\n",
    "    \"is_selected\": \"is_selected\",\n",
    "    \"passage_text\": \"passage_text\",\n",
    "    \"url\": \"url\",\n",
    "    \"query\": \"query\",\n",
    "    \"query_id\": \"query_id\",\n",
    "    \"query_type\": \"query_type\"\n",
    "})\n",
    "\n",
    "# Convert the Pandas DataFrame to a Polars DataFrame\n",
    "dfc = pl.from_pandas(pd.DataFrame(dfc))\n",
    "\n",
    "# Get unique queries\n",
    "unique_queries = dfc.select(\"query\").unique()[\"query\"].to_list()\n",
    "\n",
    "# Create empty lists to hold new data\n",
    "query_list = []\n",
    "query_id_list = []\n",
    "relevant_doc_list = []\n",
    "irrelevant_doc_list = []\n",
    "relevant_doc_is_selected_list = []\n",
    "\n",
    "# Iterate through each unique query\n",
    "num_queries = len(unique_queries)\n",
    "for idx, query in enumerate(unique_queries):\n",
    "    # Print progress every 10%\n",
    "    if idx % (num_queries // 10) == 0:\n",
    "        print(f\"Progress: {idx / num_queries * 100:.1f}% completed\")\n",
    "\n",
    "    # Get all relevant documents for the current query\n",
    "    query_relevant_docs = dfc.filter(pl.col(\"query\") == query)\n",
    "    # Get irrelevant documents (documents that belong to other queries)\n",
    "    irrelevant_docs = dfc.filter(pl.col(\"query\") != query)\n",
    "\n",
    "    # Sample an irrelevant document for each relevant document\n",
    "    sampled_irrelevant_docs = irrelevant_docs.sample(len(query_relevant_docs))\n",
    "\n",
    "    # Combine relevant and irrelevant documents\n",
    "    query_list.extend([query] * len(query_relevant_docs))\n",
    "    query_id_list.extend(query_relevant_docs[\"query_id\"].to_list())\n",
    "    relevant_doc_list.extend(query_relevant_docs[\"passage_text\"].to_list())\n",
    "    irrelevant_doc_list.extend(sampled_irrelevant_docs[\"passage_text\"].to_list())\n",
    "    relevant_doc_is_selected_list.extend(query_relevant_docs[\"is_selected\"].to_list())\n",
    "\n",
    "# Create final Polars DataFrame\n",
    "final_df = pl.DataFrame({\n",
    "    \"query\": query_list,\n",
    "    \"query_id\": query_id_list,\n",
    "    \"relevant_doc\": relevant_doc_list,\n",
    "    \"irrelevant_doc\": irrelevant_doc_list,\n",
    "    \"relevant_doc_is_selected\": relevant_doc_is_selected_list\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>query</th><th>query_id</th><th>relevant_doc</th><th>irrelevant_doc</th><th>relevant_doc_is_selected</th></tr><tr><td>str</td><td>i64</td><td>str</td><td>str</td><td>i64</td></tr></thead><tbody><tr><td>&quot;what is the two subunits of a …</td><td>81114</td><td>&quot;Each ribosome is divided into …</td><td>&quot;The appearance of scabs after …</td><td>0</td></tr><tr><td>&quot;what is the two subunits of a …</td><td>81114</td><td>&quot;The name of the DNA subunit is…</td><td>&quot;Bronchospasms are a possible s…</td><td>0</td></tr><tr><td>&quot;what is the two subunits of a …</td><td>81114</td><td>&quot;Conceptually, the circuitry of…</td><td>&quot;Menopause is a natural part of…</td><td>0</td></tr><tr><td>&quot;what is the two subunits of a …</td><td>81114</td><td>&quot;An arithmetic-logic unit (ALU)…</td><td>&quot;gasoline prices in saudi arabi…</td><td>0</td></tr><tr><td>&quot;what is the two subunits of a …</td><td>81114</td><td>&quot;The two basic units of a CPU, …</td><td>&quot;Stefanie Powers is an American…</td><td>0</td></tr><tr><td>&quot;what is the two subunits of a …</td><td>81114</td><td>&quot;The CPU itself is comprised of…</td><td>&quot;1 Kits typically do not includ…</td><td>1</td></tr><tr><td>&quot;what is the two subunits of a …</td><td>81114</td><td>&quot;The central processing unit is…</td><td>&quot;People sold what they purporte…</td><td>0</td></tr><tr><td>&quot;what is the two subunits of a …</td><td>81114</td><td>&quot;1 The central processing unit(…</td><td>&quot;A demonstration of LG&#x27;s MediaL…</td><td>0</td></tr><tr><td>&quot;what is the two subunits of a …</td><td>81114</td><td>&quot;Figure 4.1 explains the genera…</td><td>&quot;The best-paid 10 percent made …</td><td>0</td></tr><tr><td>&quot;is it normal to freeze fish at…</td><td>30560</td><td>&quot;Fish and seafood begins to dec…</td><td>&quot;Answers. Best Answer: Kingdom:…</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 5)\n",
       "┌─────────────────┬──────────┬───────────────────────┬──────────────────────┬──────────────────────┐\n",
       "│ query           ┆ query_id ┆ relevant_doc          ┆ irrelevant_doc       ┆ relevant_doc_is_sele │\n",
       "│ ---             ┆ ---      ┆ ---                   ┆ ---                  ┆ cted                 │\n",
       "│ str             ┆ i64      ┆ str                   ┆ str                  ┆ ---                  │\n",
       "│                 ┆          ┆                       ┆                      ┆ i64                  │\n",
       "╞═════════════════╪══════════╪═══════════════════════╪══════════════════════╪══════════════════════╡\n",
       "│ what is the two ┆ 81114    ┆ Each ribosome is      ┆ The appearance of    ┆ 0                    │\n",
       "│ subunits of a … ┆          ┆ divided into …        ┆ scabs after …        ┆                      │\n",
       "│ what is the two ┆ 81114    ┆ The name of the DNA   ┆ Bronchospasms are a  ┆ 0                    │\n",
       "│ subunits of a … ┆          ┆ subunit is…           ┆ possible s…          ┆                      │\n",
       "│ what is the two ┆ 81114    ┆ Conceptually, the     ┆ Menopause is a       ┆ 0                    │\n",
       "│ subunits of a … ┆          ┆ circuitry of…         ┆ natural part of…     ┆                      │\n",
       "│ what is the two ┆ 81114    ┆ An arithmetic-logic   ┆ gasoline prices in   ┆ 0                    │\n",
       "│ subunits of a … ┆          ┆ unit (ALU)…           ┆ saudi arabi…         ┆                      │\n",
       "│ what is the two ┆ 81114    ┆ The two basic units   ┆ Stefanie Powers is   ┆ 0                    │\n",
       "│ subunits of a … ┆          ┆ of a CPU, …           ┆ an American…         ┆                      │\n",
       "│ what is the two ┆ 81114    ┆ The CPU itself is     ┆ 1 Kits typically do  ┆ 1                    │\n",
       "│ subunits of a … ┆          ┆ comprised of…         ┆ not includ…          ┆                      │\n",
       "│ what is the two ┆ 81114    ┆ The central           ┆ People sold what     ┆ 0                    │\n",
       "│ subunits of a … ┆          ┆ processing unit is…   ┆ they purporte…       ┆                      │\n",
       "│ what is the two ┆ 81114    ┆ 1 The central         ┆ A demonstration of   ┆ 0                    │\n",
       "│ subunits of a … ┆          ┆ processing unit(…     ┆ LG's MediaL…         ┆                      │\n",
       "│ what is the two ┆ 81114    ┆ Figure 4.1 explains   ┆ The best-paid 10     ┆ 0                    │\n",
       "│ subunits of a … ┆          ┆ the genera…           ┆ percent made …       ┆                      │\n",
       "│ is it normal to ┆ 30560    ┆ Fish and seafood      ┆ Answers. Best        ┆ 0                    │\n",
       "│ freeze fish at… ┆          ┆ begins to dec…        ┆ Answer: Kingdom:…    ┆                      │\n",
       "└─────────────────┴──────────┴───────────────────────┴──────────────────────┴──────────────────────┘"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.write_parquet(\"query_rel_doc.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Initialize stemmer and stopwords\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_list(tokens: list[str]) -> list[str]:\n",
    "    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def preprocess_str(text: str) -> list[str]:\n",
    "    if text is None or pd.isna(text):\n",
    "        return []\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(f\"[{string.punctuation}]\", \"\", text)\n",
    "    tokens = simple_preprocess(\n",
    "        text, deacc=True\n",
    "    )  # deacc=True removes accents and punctuations\n",
    "\n",
    "    tokens = preprocess_list(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa = final_df.to_pandas()\n",
    "dfa = dfa[[\"query_id\", \"query\", \"relevant_doc\", \"irrelevant_doc\"]]\n",
    "dfa.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the strings, from each column, into list of tokens\n",
    "dfa.loc[:, \"query_tokens\"] = dfa[\"query\"].apply(preprocess_str)\n",
    "dfa.loc[:, \"relevant_document_tokens\"] = dfa[\"relevant_doc\"].apply(preprocess_str)\n",
    "dfa.loc[:, \"irrelevant_document_tokens\"] = dfa[\"irrelevant_doc\"].apply(\n",
    "    preprocess_str\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa.to_parquet(\"list_of_tokens.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa = pd.read_parquet(\"list_of_tokens.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.3)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.5)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([74792, 100])\n"
     ]
    }
   ],
   "source": [
    "w2v = gensim.models.Word2Vec.load(\n",
    "    \"word2vec-gensim-text8-custom-preprocess.model\"\n",
    ")\n",
    "\n",
    "vocab = w2v.wv.index_to_key\n",
    "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
    "embeddings_array = np.array([w2v.wv[word] for word in vocab])\n",
    "embeddings = torch.tensor(embeddings_array, dtype=torch.float32)\n",
    "print(embeddings.shape)\n",
    "\n",
    "embedding_layer = nn.Embedding.from_pretrained(embeddings, freeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_tokens(tokens: list[str], unknown_tokens: set):\n",
    "    valid_tokens = [token for token in tokens if token in word_to_idx]\n",
    "    unknown_tokens.update(set(tokens) - set(valid_tokens))\n",
    "    if valid_tokens:\n",
    "        return (\n",
    "            embedding_layer(\n",
    "                torch.tensor(\n",
    "                    [word_to_idx[token] for token in valid_tokens], dtype=torch.long\n",
    "                )\n",
    "            ),\n",
    "            unknown_tokens,\n",
    "        )\n",
    "    return torch.tensor([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_tokens = set()\n",
    "\n",
    "dfa.loc[:, \"query_embedding\"] = dfa[\"query_tokens\"].apply(\n",
    "    embed_tokens, args=(unknown_tokens,)\n",
    ")\n",
    "dfa.loc[:, \"relevant_document_embedding\"] = dfa[\"relevant_document_tokens\"].apply(\n",
    "    embed_tokens, args=(unknown_tokens,)\n",
    ")\n",
    "dfa.loc[:, \"irrelevant_document_embedding\"] = dfa[\"irrelevant_document_tokens\"].apply(\n",
    "    embed_tokens, args=(unknown_tokens,)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with empty embeddings\n",
    "dfb = dfa[\n",
    "    dfa[\"query_embedding\"].apply(\n",
    "        lambda x: len(x[0]) > 0 if isinstance(x, tuple) else len(x) > 0\n",
    "    )\n",
    "    & dfa[\"relevant_document_embedding\"].apply(\n",
    "        lambda x: len(x[0]) > 0 if isinstance(x, tuple) else len(x) > 0\n",
    "    )\n",
    "    & dfa[\"irrelevant_document_embedding\"].apply(\n",
    "        lambda x: len(x[0]) > 0 if isinstance(x, tuple) else len(x) > 0\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unknown tokens: 234255\n",
      "Some unknown tokens: ['alsothət', 'slumdog', 'modulatedmad', 'γαια', 'crorun', 'tohuman', 'slimacceler', 'cagepotato', 'antifeminist', 'paramu']\n"
     ]
    }
   ],
   "source": [
    "# print the number of unknown tokens\n",
    "print(f\"Number of unknown tokens: {len(unknown_tokens)}\")\n",
    "# print the first 10 unknown tokens\n",
    "print(f\"Some unknown tokens: {list(unknown_tokens)[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embeddings = []\n",
    "rel_doc_embeddings = []\n",
    "irrel_doc_embeddings = []\n",
    "\n",
    "max_query_len = 0\n",
    "max_rel_doc_len = 0\n",
    "max_irrel_doc_len = 0\n",
    "\n",
    "for _, row in dfb.iterrows():\n",
    "    query_emb = row['query_embedding'][0]\n",
    "    rel_doc_emb = row['relevant_document_embedding'][0]\n",
    "    irrel_doc_emb = row['irrelevant_document_embedding'][0]\n",
    "    \n",
    "    max_query_len = max(max_query_len, query_emb.shape[0])\n",
    "    max_rel_doc_len = max(max_rel_doc_len, rel_doc_emb.shape[0])\n",
    "    max_irrel_doc_len = max(max_irrel_doc_len, irrel_doc_emb.shape[0])\n",
    "    \n",
    "    query_embeddings.append(query_emb)\n",
    "    rel_doc_embeddings.append(rel_doc_emb)\n",
    "    irrel_doc_embeddings.append(irrel_doc_emb)\n",
    "\n",
    "# Pad tensors to the maximum length\n",
    "def pad_tensor(tensor, max_len):\n",
    "    padding = torch.zeros(max_len - tensor.shape[0], tensor.shape[1])\n",
    "    return torch.cat([tensor, padding], dim=0)\n",
    "\n",
    "query_embeddings = torch.stack([pad_tensor(emb, max_query_len) for emb in query_embeddings])\n",
    "rel_doc_embeddings = torch.stack([pad_tensor(emb, max_rel_doc_len) for emb in rel_doc_embeddings])\n",
    "irrel_doc_embeddings = torch.stack([pad_tensor(emb, max_irrel_doc_len) for emb in irrel_doc_embeddings])\n",
    "\n",
    "# Print shapes to verify\n",
    "print(f\"Query embeddings shape: {query_embeddings.shape}\")\n",
    "print(f\"Relevant document embeddings shape: {rel_doc_embeddings.shape}\")\n",
    "print(f\"Irrelevant document embeddings shape: {irrel_doc_embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tensors to a file\n",
    "file_path = os.path.join(\"PreRNN_embeddings.pt\")\n",
    "torch.save({\n",
    "    'query_embeddings': query_embeddings,\n",
    "    'rel_doc_embeddings': rel_doc_embeddings,\n",
    "    'irrel_doc_embeddings': irrel_doc_embeddings\n",
    "}, file_path)\n",
    "\n",
    "print(f\"Embeddings saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the data:\n",
    "# loaded_data = torch.load(file_path)\n",
    "# query_embeddings = loaded_data['query_embeddings']\n",
    "# rel_doc_embeddings = loaded_data['rel_doc_embeddings']\n",
    "# irrel_doc_embeddings = loaded_data['irrel_doc_embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
