{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from gensim.utils import simple_preprocess\n",
    "import collections\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis script:\\n- imports the train.parquet form data/\\n- preprocesses the queries and docs\\n- creates lookup tables for the vocab\\n- tokenizes the queries and docs\\n- saves the processed queries and docs as pickle files in data/processed/\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script:\n",
    "- imports the train.parquet form data/\n",
    "- preprocesses the queries and docs\n",
    "- creates lookup tables for the vocab\n",
    "- tokenizes the queries and docs\n",
    "- saves the processed queries and docs as pickle files in data/processed/\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(\"data\", \"train.parquet\")\n",
    "df = pd.read_parquet(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kaleb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kaleb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Initialize stemmer and stopwords\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_query(query: str) -> list[str]:\n",
    "    if query is None or pd.isna(query):\n",
    "        return []\n",
    "\n",
    "    query = query.lower()\n",
    "\n",
    "    query = re.sub(f\"[{string.punctuation}]\", \"\", query)\n",
    "\n",
    "    tokens = simple_preprocess(\n",
    "        query, deacc=True\n",
    "    )  # deacc=True removes accents and punctuations\n",
    "\n",
    "    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
    "\n",
    "    tokens = tokens + [\"[<EOS>]\"]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lookup_tables(words: list[str]) -> tuple[dict[str, int], dict[int, str]]:\n",
    "    word_counts = collections.Counter(words)\n",
    "    vocab = sorted(word_counts, key=lambda k: word_counts.get(k), reverse=True)\n",
    "    int_to_vocab = {ii + 1: word for ii, word in enumerate(vocab)}\n",
    "    int_to_vocab[0] = \"<PAD>\"\n",
    "    vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meaning of propagation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                                   [[S], rba, [E]]\n",
       "1              [[S], ronald, reagan, democrat, [E]]\n",
       "2    [[S], long, need, sydney, surround, area, [E]]\n",
       "3           [[S], price, instal, tile, shower, [E]]\n",
       "4                 [[S], convers, observ, bodi, [E]]\n",
       "Name: query, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_queries = df[\"query\"].apply(\n",
    "    lambda x: preprocess_query(x) if pd.notna(x) else []\n",
    ")\n",
    "\n",
    "print(df[\"query\"][82321])\n",
    "processed_queries[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_corpus = [word for query in processed_queries for word in query]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_to_int, int_to_vocab = create_lookup_tables(query_corpus)\n",
    "tokenized_queries = [\n",
    "    [vocab_to_int[word] for word in query] for query in processed_queries\n",
    "]\n",
    "query_dataset = (processed_queries, tokenized_queries, vocab_to_int, int_to_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/processed/queries_dataset.pkl\", \"wb\") as f:\n",
    "    pickle.dump(query_dataset, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82326"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df[\"passages\"][0]\n",
    "# len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd = df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get max length of 'passage_text' (array)\n",
    "max_length = max(len(passage[\"passage_text\"]) for passage in df[\"passages\"])\n",
    "\n",
    "# Extract 'passage_text' list items into separate columns\n",
    "for i in range(max_length):\n",
    "    df[f\"passage_text_{i}\"] = dfd[\"passages\"].apply(\n",
    "        lambda x: x[\"passage_text\"][i] if i < len(x[\"passage_text\"]) else None\n",
    "    )\n",
    "\n",
    "# Apply the preprocessing function to each column of interest\n",
    "for i in range(max_length):\n",
    "    column_name = f\"passage_text_{i}\"\n",
    "    df[column_name] = df[column_name].apply(\n",
    "        lambda x: str(preprocess_query(x)) if pd.notna(x) else \"[]\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_parquet(\"data/processed/docs.parquet\")\n",
    "processed_docs = []\n",
    "\n",
    "# Consolidate all passages into processed_docs\n",
    "for i in range(max_length):\n",
    "    column_name = f\"passage_text_{i}\"\n",
    "    processed_docs += (\n",
    "        df[column_name]\n",
    "        .apply(lambda x: preprocess_query(x) if pd.notna(x) else [])  # change\n",
    "        .tolist()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"['[S]', 'sinc', 'rba', 'outstand', 'reput', 'affect', 'secur', 'npa', 'scandal', 'rba', 'subsidiari', 'involv', 'bribe', 'oversea', 'offici', 'australia', 'might', 'win', 'lucr', 'noteprint', 'contract', 'asset', 'bank', 'includ', 'gold', 'foreign', 'exchang', 'reserv', 'australia', 'estim', 'net', 'worth', 'billion', 'nearli', 'rba', 'employe', 'work', 'headquart', 'sydney', 'new', 'south', 'wale', 'busi', 'resumpt', 'site', '[E]']\",\n",
       "       \"['[S]', 'reserv', 'bank', 'australia', 'rba', 'came', 'januari', 'australia', 'central', 'bank', 'banknot', 'issu', 'author', 'reserv', 'bank', 'act', 'remov', 'central', 'bank', 'function', 'commonwealth', 'bank', 'asset', 'bank', 'includ', 'gold', 'foreign', 'exchang', 'reserv', 'australia', 'estim', 'net', 'worth', 'billion', 'nearli', 'rba', 'employe', 'work', 'headquart', 'sydney', 'new', 'south', 'wale', 'busi', 'resumpt', 'site', '[E]']\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatten processed_docs\n",
    "processed_docs = df[[f\"passage_text_{i}\" for i in range(max_length)]].values.flatten()\n",
    "processed_docs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Docs After Conversion:\n",
      "['[S]', 'sinc', 'rba', 'outstand', 'reput', 'affect', 'secur', 'npa', 'scandal', 'rba', 'subsidiari', 'involv', 'bribe', 'oversea', 'offici', 'australia', 'might', 'win', 'lucr', 'noteprint', 'contract', 'asset', 'bank', 'includ', 'gold', 'foreign', 'exchang', 'reserv', 'australia', 'estim', 'net', 'worth', 'billion', 'nearli', 'rba', 'employe', 'work', 'headquart', 'sydney', 'new', 'south', 'wale', 'busi', 'resumpt', 'site', '[E]']\n",
      "['[S]', 'reserv', 'bank', 'australia', 'rba', 'came', 'januari', 'australia', 'central', 'bank', 'banknot', 'issu', 'author', 'reserv', 'bank', 'act', 'remov', 'central', 'bank', 'function', 'commonwealth', 'bank', 'asset', 'bank', 'includ', 'gold', 'foreign', 'exchang', 'reserv', 'australia', 'estim', 'net', 'worth', 'billion', 'nearli', 'rba', 'employe', 'work', 'headquart', 'sydney', 'new', 'south', 'wale', 'busi', 'resumpt', 'site', '[E]']\n",
      "['[S]', 'rba', 'recogn', 'microsoft', 'us', 'region', 'partner', 'pr', 'newswir', 'contract', 'award', 'suppli', 'support', 'securitis', 'system', 'use', 'risk', 'manag', 'analysi', '[E]']\n",
      "['[S]', 'inner', 'work', 'rebuild', 'atom', 'surprisingli', 'simpl', 'coil', 'insid', 'rba', 'made', 'type', 'resist', 'wire', 'normal', 'kanthal', 'nichrom', 'current', 'appli', 'coil', 'resist', 'wire', 'heat', 'heat', 'coil', 'vapor', 'eliquid', 'bottom', 'feed', 'rba', 'perhap', 'easiest', 'rba', 'type', 'build', 'maintain', 'use', 'fill', 'much', 'like', 'bottom', 'coil', 'clearom', 'bottom', 'feed', 'rba', 'util', 'cotton', 'instead', 'silica', 'wick', 'genesi', 'genni', 'top', 'feed', 'rba', 'util', 'short', 'woven', 'mesh', 'wire', '[E]']\n",
      "['[S]', 'resultsbas', 'account', 'also', 'known', 'rba', 'disciplin', 'way', 'think', 'take', 'action', 'commun', 'use', 'improv', 'live', 'children', 'youth', 'famili', 'adult', 'commun', 'whole', 'rba', 'also', 'use', 'organ', 'improv', 'perform', 'program', 'rba', 'improv', 'live', 'children', 'famili', 'commun', 'perform', 'program', 'rba', 'get', 'talk', 'action', 'quickli', 'simpl', 'common', 'sens', 'process', 'everyon', 'understand', 'help', 'group', 'surfac', 'challeng', 'assumpt', 'barrier', 'innov', '[E]']\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "processed_docs = [\n",
    "    ast.literal_eval(doc) if isinstance(doc, str) else doc for doc in processed_docs\n",
    "]\n",
    "print(\"Processed Docs After Conversion:\")\n",
    "for i in range(5):\n",
    "    print(processed_docs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Docs Corpus Sample:\n",
      "['[S]', 'sinc', 'rba', 'outstand', 'reput']\n",
      "3837\n"
     ]
    }
   ],
   "source": [
    "docs_corpus = [word for doc in processed_docs for word in doc]\n",
    "print(\"\\nDocs Corpus Sample:\")\n",
    "print(docs_corpus[:5])\n",
    "print(len(docs_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_vocab_to_int, docs_int_to_vocab = create_lookup_tables(docs_corpus)\n",
    "tokenized_docs = [\n",
    "    [docs_vocab_to_int[word] for word in doc] for doc in processed_docs  # change\n",
    "]\n",
    "docs_dataset = (processed_docs, tokenized_docs, docs_vocab_to_int, docs_int_to_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/processed/docs_dataset.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open(\"data/processed/docs_dataset.pkl\", \"wb\") as f:\n",
    "    pickle.dump(docs_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
