{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import wandb\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "from utils.collate import collate\n",
    "from utils.load_data import load_word2vec\n",
    "from utils.preprocess_str import str_to_tokens, tokenize\n",
    "from utils.checkpoint import save_checkpoint\n",
    "from models.core import DocumentDataset, TwoTowerModel, loss_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "FREEZE_EMBEDDINGS = True\n",
    "# Load embeddings\n",
    "vocab,embeddings, word_to_idx = load_word2vec(embeddings_path='../models/word2vec/word2vec-gensim-text8-custom-preprocess.model')\n",
    "embedding_layer = nn.Embedding.from_pretrained(embeddings, freeze=FREEZE_EMBEDDINGS)\n",
    "\n",
    "EMBEDDING_DIM = embeddings.shape[1]\n",
    "VOCAB_SIZE = len(vocab)\n",
    "PROJECTION_DIM = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/x25bd/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/x25bd/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# # reload lib hard\n",
    "# import importlib\n",
    "# import utils\n",
    "# importlib.reload(utils.preprocess_str)\n",
    "# from utils.preprocess_str import str_to_tokens, tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load validation data\n",
    "df = pd.read_parquet('./validation-tokenized.parquet')\n",
    "\n",
    "# Only if we need to tokenize again\n",
    "# df_original = pd.read_parquet('../data/validation.parquet')\n",
    "# df_tokenized = tokenize(df_original, word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "validationset = DocumentDataset(df)\n",
    "validationloader = DataLoader(validationset, batch_size=1000, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwoTowerModel(embedding_dim=EMBEDDING_DIM, \n",
    "                        projection_dim=PROJECTION_DIM, \n",
    "                        embedding_layer=embedding_layer, \n",
    "                        margin=MARGIN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
