{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9585b91-1060-4bf5-ac46-613692378846",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import polars as pl\n",
    "import gensim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25ca74e0-8f68-4564-ae8d-4d093bebaae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([74792, 100])\n"
     ]
    }
   ],
   "source": [
    "w2v = gensim.models.Word2Vec.load(\n",
    "    \"word2vec-gensim-text8-custom-preprocess.model\"\n",
    ")\n",
    "\n",
    "vocab = w2v.wv.index_to_key\n",
    "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
    "embeddings_array = np.array([w2v.wv[word] for word in vocab])\n",
    "embeddings = torch.tensor(embeddings_array, dtype=torch.float32)\n",
    "print(embeddings.shape)\n",
    "\n",
    "embedding_layer = nn.Embedding.from_pretrained(embeddings, freeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d418ab47-c990-4bb9-bf16-56b8e26c1762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def embed_tokens(tokens: list[str], unknown_tokens: set):\n",
    "#     valid_tokens = [token for token in tokens if token in word_to_idx]\n",
    "#     unknown_tokens.update(set(tokens) - set(valid_tokens))\n",
    "#     if valid_tokens:\n",
    "#         return (\n",
    "#             embedding_layer(\n",
    "#                 torch.tensor(\n",
    "#                     [word_to_idx[token] for token in valid_tokens], dtype=torch.long\n",
    "#                 )\n",
    "#             ),\n",
    "#             unknown_tokens,\n",
    "#         )\n",
    "#     return torch.tensor([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "045490b2-4bd9-4c3e-8ff7-5d8d09506412",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa = pd.read_parquet(\"list_of_tokens.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "772e95fd-118e-4dbf-8523-a7cb4184ff3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Assuming `embedding_layer` is a PyTorch layer, move it to the GPU\n",
    "embedding_layer = embedding_layer.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e4ccbc61-7944-4c55-9450-1f13e041144a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aa4c3b67-1ebb-47ab-a07b-1f4d64fd0c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "embedding_layer = embedding_layer.to(device)\n",
    "\n",
    "def embed_tokens(tokens, unknown_tokens):\n",
    "    if tokens is None or (isinstance(tokens, (float, int)) and pd.isna(tokens)):\n",
    "        print(\"Tokens are None or NaN\")\n",
    "        return []\n",
    "\n",
    "    if isinstance(tokens, (list, pd.Series, pd.Index, np.ndarray)):\n",
    "        tokens = [token for token in tokens if pd.notna(token)]\n",
    "        if not tokens:\n",
    "            print(\"Tokens are empty after filtering NaNs\")\n",
    "            return []\n",
    "\n",
    "        valid_tokens = [token for token in tokens if token in word_to_idx]\n",
    "        unknown_tokens.update(set(tokens) - set(valid_tokens))\n",
    "        \n",
    "        if valid_tokens:\n",
    "            print(f\"Valid tokens found: {valid_tokens}\")\n",
    "            token_indices = torch.tensor([word_to_idx[token] for token in valid_tokens], dtype=torch.long).to(device)\n",
    "            embeddings = embedding_layer(token_indices)\n",
    "            return embeddings.cpu().numpy().tolist()\n",
    "        \n",
    "        print(\"No valid tokens found\")\n",
    "    else:\n",
    "        print(\"Tokens are not a valid type\")\n",
    "    \n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020ef0ea-96d5-4d77-9bea-61b9f6191239",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa = dd.read_parquet('list_of_tokens.parquet', engine='pyarrow')\n",
    "sample_size = 1000  # Set the desired sample size\n",
    "small_sample_pandas_df = dfa.head(sample_size)  # This returns a Pandas DataFrame with the first `sample_size` rows\n",
    "dfa = dd.from_pandas(small_sample_pandas_df, npartitions=1)\n",
    "\n",
    "unknown_tokens = set()\n",
    "\n",
    "dfa['query_embedding'] = dfa.map_partitions(\n",
    "    lambda df: df['query_tokens'].apply(embed_tokens, args=(unknown_tokens,)),\n",
    "    meta=('query_embedding', 'object')\n",
    ")\n",
    "\n",
    "dfa['relevant_document_embedding'] = dfa.map_partitions(\n",
    "    lambda df: df['relevant_document_tokens'].apply(embed_tokens, args=(unknown_tokens,)),\n",
    "    meta=('relevant_document_embedding', 'object')\n",
    ")\n",
    "\n",
    "dfa['irrelevant_document_embedding'] = dfa.map_partitions(\n",
    "    lambda df: df['irrelevant_document_tokens'].apply(embed_tokens, args=(unknown_tokens,)),\n",
    "    meta=('irrelevant_document_embedding', 'object')\n",
    ")\n",
    "\n",
    "with ProgressBar():\n",
    "    dfa = dfa.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacefeaa-fdea-4c5c-ab10-b0b6cef87653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now `unknown_tokens` should contain all the tokens that were not found during the embedding process\n",
    "print(f\"Number of unknown tokens: {len(unknown_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "636b18dd-8fd8-44c6-95a3-84f2b8790004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 5.09 ss\n"
     ]
    }
   ],
   "source": [
    "# Convert the Pandas DataFrame to a Dask DataFrame\n",
    "dask_df = dd.from_pandas(dfa, npartitions=10)\n",
    "\n",
    "# Define function to check if the embedding is non-empty\n",
    "def is_non_empty(x):\n",
    "    if isinstance(x, tuple):\n",
    "        return len(x[0]) > 0\n",
    "    else:\n",
    "        return len(x) > 0\n",
    "\n",
    "# Apply filtering to each partition to remove rows with empty embeddings\n",
    "dfa_filtered = dask_df.map_partitions(\n",
    "    lambda df: df.loc[\n",
    "        df[\"query_embedding\"].apply(is_non_empty)\n",
    "        & df[\"relevant_document_embedding\"].apply(is_non_empty)\n",
    "        & df[\"irrelevant_document_embedding\"].apply(is_non_empty)\n",
    "    ],\n",
    "    meta=dask_df._meta  # Use the Dask DataFrame's _meta attribute\n",
    ")\n",
    "\n",
    "# Compute the filtered dataframe\n",
    "with ProgressBar():\n",
    "    dfb = dfa_filtered.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "25a8b5b1-bf72-4c1b-9676-c91ee3e3984e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>query_embedding</th>\n",
       "      <th>relevant_document_embedding</th>\n",
       "      <th>irrelevant_document_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>81114</td>\n",
       "      <td>[[-0.5350781083106995, -0.31560149788856506, 0...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>81114</td>\n",
       "      <td>[[-0.5350781083106995, -0.31560149788856506, 0...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>81114</td>\n",
       "      <td>[[-0.5350781083106995, -0.31560149788856506, 0...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>81114</td>\n",
       "      <td>[[-0.5350781083106995, -0.31560149788856506, 0...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>81114</td>\n",
       "      <td>[[-0.5350781083106995, -0.31560149788856506, 0...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>81114</td>\n",
       "      <td>[[-0.5350781083106995, -0.31560149788856506, 0...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>81114</td>\n",
       "      <td>[[-0.5350781083106995, -0.31560149788856506, 0...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>81114</td>\n",
       "      <td>[[-0.5350781083106995, -0.31560149788856506, 0...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>81114</td>\n",
       "      <td>[[-0.5350781083106995, -0.31560149788856506, 0...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>30560</td>\n",
       "      <td>[[0.15981212258338928, -0.5535925030708313, -0...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query_id                                    query_embedding  \\\n",
       "0     81114  [[-0.5350781083106995, -0.31560149788856506, 0...   \n",
       "1     81114  [[-0.5350781083106995, -0.31560149788856506, 0...   \n",
       "2     81114  [[-0.5350781083106995, -0.31560149788856506, 0...   \n",
       "3     81114  [[-0.5350781083106995, -0.31560149788856506, 0...   \n",
       "4     81114  [[-0.5350781083106995, -0.31560149788856506, 0...   \n",
       "5     81114  [[-0.5350781083106995, -0.31560149788856506, 0...   \n",
       "6     81114  [[-0.5350781083106995, -0.31560149788856506, 0...   \n",
       "7     81114  [[-0.5350781083106995, -0.31560149788856506, 0...   \n",
       "8     81114  [[-0.5350781083106995, -0.31560149788856506, 0...   \n",
       "9     30560  [[0.15981212258338928, -0.5535925030708313, -0...   \n",
       "\n",
       "  relevant_document_embedding irrelevant_document_embedding  \n",
       "0                          []                            []  \n",
       "1                          []                            []  \n",
       "2                          []                            []  \n",
       "3                          []                            []  \n",
       "4                          []                            []  \n",
       "5                          []                            []  \n",
       "6                          []                            []  \n",
       "7                          []                            []  \n",
       "8                          []                            []  \n",
       "9                          []                            []  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfb[\n",
    "    [\n",
    "        \"query_id\",\n",
    "        \"query_embedding\",\n",
    "        \"relevant_document_embedding\",\n",
    "        \"irrelevant_document_embedding\",\n",
    "    ]\n",
    "].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb868f0c-eac2-4923-8737-2e02057ea925",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcbe460-a0f2-415e-8aff-e0f7452523d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0c1f04-ebf9-445d-a5b4-d2e569df74cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
